## Course Overview

**Duration:** 8-10 hours  
**Level:** Intermediate  
**Prerequisites:** Basic understanding of networking, operating systems, and security concepts

### Learning Objectives

By the end of this course, you will be able to:
- Understand the role of log analysis in digital forensics investigations
- Identify and collect logs from various sources (network devices, servers, applications)
- Analyze different log formats and extract relevant forensic evidence
- Correlate log data across multiple sources to reconstruct events
- Apply best practices for log preservation and chain of custody
- Use common tools and techniques for efficient log analysis

---

## Module 1: Introduction to Log Analysis in Digital Forensics

### 1.1 What is Log Analysis?

Log analysis is the systematic examination of log files generated by computer systems, network devices, and applications to identify security incidents, troubleshoot issues, or reconstruct events for forensic purposes. In digital forensics, logs serve as digital evidence that can:

- Establish timelines of events
- Identify unauthorized access or suspicious activities
- Prove or disprove allegations
- Reveal attack patterns and methods
- Support incident response efforts

**Connection to Digital Forensics:** Logs are often the only persistent record of system and network activity. Unlike volatile memory or temporary files, logs are specifically designed to record events, making them invaluable for forensic investigations. They provide the "what, when, who, and where" of digital incidents.

### 1.2 Types of Logs in Digital Forensics

#### System Logs
- Operating system events (Windows Event Logs, Linux syslog)
- Authentication attempts (successful and failed logins)
- System changes (software installations, configuration changes)
- Resource usage and performance metrics

#### Network Logs
- Firewall logs (allowed/blocked connections)
- Router and switch logs (routing decisions, interface status)
- VPN logs (remote access sessions)
- IDS/IPS logs (detected threats and prevention actions)

#### Application Logs
- Web server logs (HTTP requests, responses, errors)
- Database logs (queries, transactions, access attempts)
- Email server logs (message routing, delivery status)
- Custom application logs (business logic events)

#### Security Logs
- Antivirus/anti-malware logs (detections, quarantine actions)
- DLP (Data Loss Prevention) logs (data transfers, policy violations)
- SIEM (Security Information and Event Management) aggregated logs
- Authentication server logs (Active Directory, RADIUS, LDAP)

**Connection to Main Topic:** Understanding log types is fundamental because each source provides unique information. A comprehensive forensic analysis requires correlating multiple log sources to build a complete picture of an incident. No single log type tells the entire story.

### 1.3 Legal and Regulatory Considerations

When conducting log analysis for forensic purposes, investigators must consider:

- **Chain of Custody:** Documenting who collected, accessed, and analyzed logs
- **Data Integrity:** Ensuring logs haven't been tampered with (using hashes, write-once media)
- **Time Synchronization:** Verifying accurate timestamps across sources (NTP importance)
- **Privacy Regulations:** Compliance with GDPR, HIPAA, or other data protection laws
- **Admissibility:** Following proper procedures to ensure logs can be used as evidence in court

---

## Module 2: Log Collection and Preservation

### 2.1 Principles of Log Collection

**Forensic Soundness:** The collection process must not alter the original evidence. Follow these principles:

1. **Document Everything:** Record what logs were collected, when, how, and by whom
2. **Use Write-Protected Media:** Prevent accidental modification during collection
3. **Create Cryptographic Hashes:** Generate MD5/SHA256 hashes to verify integrity
4. **Maintain Chain of Custody:** Create detailed logs of evidence handling
5. **Preserve Timestamps:** Note the time zone and ensure accurate time correlation

**Connection to Main Topic:** Improper collection can render logs inadmissible in court or lead to incorrect conclusions. The collection phase establishes the foundation for all subsequent analysis, making it critical to forensic success.

### 2.2 Collecting Network Device Logs

#### Firewall Logs

**What They Contain:**
- Source and destination IP addresses
- Port numbers and protocols
- Action taken (allow/deny)
- Timestamps of connection attempts
- Rule IDs that triggered the action

**Collection Methods:**

**Via CLI (Command Line Interface):**
```bash
# Cisco ASA Example
show logging
copy /noconfirm logging ftp://username@ftpserver/firewall_logs.txt

# Palo Alto Example
show log system
scp export log-file management.log from mgmt to user@scpserver:path
```

**Via Management Interface:**
- Access the firewall's web GUI
- Navigate to Monitoring â†’ Logs
- Export logs (typically CSV, CEF, or proprietary format)
- Save to forensically sound media

**Via Syslog Server:**
- If centralized logging is configured, collect from the syslog server
- Preserves logs that may have been rotated from the device
- Example collection: `rsync -av /var/log/firewall/ /mnt/evidence/`

**Best Practices:**
- Collect logs before and after the incident window (context is crucial)
- Note the firewall's time zone and NTP configuration
- Document the device configuration (especially logging settings)
- If possible, collect configuration backups showing logging was enabled

#### Router and Switch Logs

**What They Contain:**
- Interface status changes
- Routing protocol messages
- Access control list (ACL) hits
- SNMP trap messages
- Hardware errors

**Collection Example (Cisco IOS):**
```bash
show logging
show logging history
terminal length 0  # Disable paging
show logging | redirect flash:router_logs.txt
copy flash:router_logs.txt tftp://server/router_logs.txt
```

**Connection to Network Forensics:** Network device logs reveal the path of network traffic, can show lateral movement during attacks, and provide evidence of network-based intrusions. They're essential for understanding how attackers moved through the network infrastructure.

### 2.3 Collecting Server Logs

#### Windows Server Logs

**Log Locations:**
- Event Logs: `%SystemRoot%\System32\winevt\Logs\`
- IIS Logs: `%SystemDrive%\inetpub\logs\LogFiles\`
- Application-specific: Varies by application

**Collection Using Built-in Tools:**

**PowerShell Method:**
```powershell
# Export Security Event Log
Get-WinEvent -LogName Security | Export-Csv C:\evidence\security_log.csv

# Export with specific date range
$StartDate = (Get-Date).AddDays(-7)
Get-WinEvent -FilterHashtable @{LogName='Security'; StartTime=$StartDate} | 
    Export-Csv C:\evidence\security_filtered.csv

# Save raw .evtx file
wevtutil epl Security C:\evidence\security.evtx
```

**Using wevtutil:**
```cmd
wevtutil epl System C:\evidence\system.evtx
wevtutil epl Application C:\evidence\application.evtx
wevtutil epl Security C:\evidence\security.evtx
```

**Key Windows Event IDs for Forensics:**
- 4624: Successful logon
- 4625: Failed logon attempt
- 4648: Logon using explicit credentials
- 4672: Special privileges assigned to new logon
- 4720: User account created
- 4732: Member added to security-enabled local group
- 7045: Service installed
- 4688: New process created (if enabled)

#### Linux Server Logs

**Log Locations:**
- `/var/log/syslog` or `/var/log/messages` (general system log)
- `/var/log/auth.log` or `/var/log/secure` (authentication)
- `/var/log/apache2/` or `/var/log/httpd/` (web server)
- `/var/log/mysql/` (database)
- `journalctl` output (systemd systems)

**Collection Commands:**
```bash
# Create evidence directory with timestamp
EVIDENCE_DIR="/mnt/evidence/$(hostname)_$(date +%Y%m%d_%H%M%S)"
mkdir -p $EVIDENCE_DIR

# Copy log files preserving timestamps
cp -p /var/log/syslog* $EVIDENCE_DIR/
cp -p /var/log/auth.log* $EVIDENCE_DIR/
cp -p /var/log/apache2/* $EVIDENCE_DIR/apache2/

# Export systemd journal
journalctl --since "2025-01-01" --until "2025-01-31" > $EVIDENCE_DIR/journal.log

# Generate hash manifest
find $EVIDENCE_DIR -type f -exec sha256sum {} \; > $EVIDENCE_DIR/hash_manifest.txt

# Create compressed archive
tar -czf evidence_$(hostname)_$(date +%Y%m%d).tar.gz $EVIDENCE_DIR
```

**Connection to Main Topic:** Server logs are often the most detailed source of information about user activities, application behavior, and system changes. They're central to forensic investigations involving data breaches, insider threats, and system compromises.

### 2.4 Collecting Application Logs

#### Web Server Logs

**Apache Access Log Format (Common Log Format):**
```
127.0.0.1 - frank [10/Oct/2025:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 2326
```

**Fields:**
- IP address
- Identity (usually -)
- Username (from HTTP authentication)
- Timestamp
- HTTP method and requested resource
- HTTP status code
- Bytes sent

**Collection:**
```bash
# Apache
cp -p /var/log/apache2/access.log* /mnt/evidence/
cp -p /var/log/apache2/error.log* /mnt/evidence/

# Nginx
cp -p /var/log/nginx/access.log* /mnt/evidence/
cp -p /var/log/nginx/error.log* /mnt/evidence/

# IIS (Windows)
Copy-Item "C:\inetpub\logs\LogFiles\W3SVC1\*" "E:\evidence\iis_logs\"
```

**IIS Log Format (W3C Extended):**
```
2025-10-06 13:55:36 192.168.1.100 GET /index.html - 80 - 203.0.113.45 Mozilla/5.0 200 0 0 125
```

#### Database Logs

**MySQL/MariaDB:**
```bash
# General query log location (if enabled)
cp -p /var/lib/mysql/hostname.log /mnt/evidence/

# Binary logs (replication/point-in-time recovery)
cp -p /var/lib/mysql/mysql-bin.* /mnt/evidence/

# Error log
cp -p /var/log/mysql/error.log /mnt/evidence/
```

**PostgreSQL:**
```bash
# Log location (varies by configuration)
cp -p /var/log/postgresql/postgresql-*.log /mnt/evidence/

# Show current log file location
sudo -u postgres psql -c "SHOW log_directory;"
sudo -u postgres psql -c "SHOW log_filename;"
```

**Microsoft SQL Server:**
```powershell
# SQL Server Error Logs
Copy-Item "C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\MSSQL\Log\ERRORLOG*" `
    "E:\evidence\sql_logs\"

# Query to retrieve SQL Server Audit logs (if configured)
Invoke-Sqlcmd -Query "SELECT * FROM sys.fn_get_audit_file('C:\SQLAudit\*.sqlaudit', DEFAULT, DEFAULT)" `
    | Export-Csv "E:\evidence\sql_audit.csv"
```

**Connection to Application Forensics:** Application logs reveal business logic execution, data access patterns, and user interactions with applications. They're critical for investigations involving data theft, fraud, or unauthorized transactions.

### 2.5 Log Preservation Best Practices

1. **Immediate Collection:** Collect logs as soon as an incident is detected (before rotation)
2. **Use Forensic Tools:** Consider tools like FTK Imager, dd, or dc3dd for disk imaging if logs are on local storage
3. **Document Configuration:** Note log retention settings, rotation policies, and time synchronization
4. **Verify Completeness:** Check for gaps in timestamps that might indicate log tampering or system downtime
5. **Secure Storage:** Store collected logs on write-once media or in secure, access-controlled locations
6. **Multiple Copies:** Create working copies for analysis; preserve originals untouched

---

## Module 3: Log Analysis Techniques

### 3.1 Understanding Log Formats

#### Structured vs. Unstructured Logs

**Structured Logs:** Follow a defined schema (JSON, CSV, XML)
```json
{
  "timestamp": "2025-10-06T13:55:36Z",
  "level": "INFO",
  "source_ip": "192.168.1.100",
  "action": "login_success",
  "username": "jdoe"
}
```

**Unstructured Logs:** Free-form text requiring parsing
```
Oct  6 13:55:36 webserver sshd[12345]: Accepted password for jdoe from 192.168.1.100 port 52834 ssh2
```

**Connection to Main Topic:** Understanding log formats is essential because forensic analysis requires extracting specific fields (timestamps, IP addresses, usernames) from diverse sources. Different formats require different parsing approaches, and misunderstanding formats can lead to missed evidence.

#### Common Log Formats

**Syslog (RFC 5424):**
```
<34>1 2025-10-06T13:55:36.123456+00:00 hostname appname 12345 ID47 - Message text
```

**CEF (Common Event Format):**
```
CEF:0|Security|IDS|1.0|100|Attempted Login|5|src=192.168.1.100 dst=10.0.0.5 suser=admin
```

**JSON (JavaScript Object Notation):**
- Native format for many modern applications
- Easy to parse programmatically
- Supports nested structures

**Windows Event Log XML:**
- Structured format with defined schema
- Queryable using XPath
- Contains rich metadata

### 3.2 Timeline Analysis

Timeline analysis involves arranging events chronologically to understand the sequence of actions during an incident.

**Steps for Timeline Creation:**

1. **Normalize Timestamps:** Convert all logs to UTC for consistency
2. **Identify Key Events:** Focus on events relevant to the investigation
3. **Correlate Across Sources:** Match events from different logs using timestamps and identifiers
4. **Fill Gaps:** Look for missing time periods that might indicate log tampering or system issues
5. **Visualize:** Create a visual timeline to spot patterns

**Example Timeline:**
```
2025-10-06 13:45:00 | Firewall   | External IP 203.0.113.45 attempted connection to port 22
2025-10-06 13:45:05 | SSH Server | Failed login attempt for user 'admin' from 203.0.113.45
2025-10-06 13:45:10 | SSH Server | Failed login attempt for user 'root' from 203.0.113.45
2025-10-06 13:45:15 | Firewall   | Blocked 203.0.113.45 (rate limiting triggered)
2025-10-06 13:50:00 | IDS        | Brute force attack detected from 203.0.113.45
```

**Tools for Timeline Analysis:**
- Plaso/Log2timeline (automated timeline creation)
- Timesketch (web-based timeline analysis)
- Excel/LibreOffice Calc (manual analysis)
- Splunk/ELK Stack (enterprise tools)

**Connection to Main Topic:** Timeline analysis is core to forensic investigations because it reveals the attack progression, identifies initial compromise vectors, and establishes the full scope of an incident. Without proper timeline analysis, investigators might miss critical events or misunderstand the sequence of attacker actions.

### 3.3 Pattern Recognition and Anomaly Detection

#### Common Attack Patterns in Logs

**Brute Force Attacks:**
- Multiple failed login attempts from same source
- Attempts across many usernames
- Pattern: High frequency, sequential attempts

**Example grep command:**
```bash
grep "Failed password" /var/log/auth.log | awk '{print $1,$2,$3,$11}' | sort | uniq -c | sort -rn
```

**SQL Injection Attempts:**
- Web server logs showing SQL keywords in URLs
- Pattern: Unusual characters (', --, ;) in query parameters
- HTTP 500 errors or database error messages

**Example pattern:**
```
192.168.1.100 - - [06/Oct/2025:14:23:45 -0700] 
"GET /product.php?id=1' OR '1'='1 HTTP/1.1" 200 5432
```

**Lateral Movement:**
- Account logging into multiple systems
- Use of administrative tools (psexec, WMI)
- Access to file shares or domain controllers

**Data Exfiltration:**
- Large outbound data transfers
- Connections to unusual external IPs
- File compression or archiving before transfer
- Access to sensitive directories followed by network activity

#### Statistical Anomaly Detection

**Baseline Normal Behavior:**
- Average login times for users
- Typical data transfer volumes
- Standard processes running on systems
- Normal geographic locations for access

**Detect Deviations:**
- Login at unusual time (3 AM when user normally works 9-5)
- Data transfer 100x larger than typical
- New process never seen before
- Access from foreign country

**Connection to Main Topic:** Pattern recognition transforms raw log data into actionable intelligence. Forensic investigators must distinguish between normal activity and indicators of compromise. Automated pattern detection helps scale analysis across millions of log entries.

### 3.4 Correlation Across Multiple Sources

Effective log analysis requires correlating information from multiple sources to build a complete picture.

**Correlation Keys:**
- **Usernames:** Track a specific account across systems
- **IP Addresses:** Follow network connections
- **Timestamps:** Match events occurring simultaneously
- **Session IDs:** Track a specific user session
- **Transaction IDs:** Follow multi-step processes

**Example Correlation Scenario:**

Investigating a data breach:

1. **Firewall Log:** Shows inbound connection from IP 203.0.113.45 to web server on port 443
2. **Web Server Log:** Shows successful login by user 'contractor_bob' from 203.0.113.45
3. **Application Log:** Shows 'contractor_bob' accessed sensitive customer database
4. **Database Log:** Shows SELECT query extracting 50,000 customer records
5. **Web Server Log:** Shows large POST request from web server to external IP
6. **Firewall Log:** Shows 2GB outbound connection to known malicious IP
7. **DLP Log:** Shows policy violation - PII data transmitted externally

**Correlation reveals:** Contractor account was compromised, used to access database, exfiltrate data.

**Tools for Correlation:**
- SIEM platforms (Splunk, QRadar, LogRhythm)
- Log analysis frameworks (ELK Stack)
- Custom scripts (Python, PowerShell)
- Spreadsheet analysis for small datasets

### 3.5 Handling Large Log Files

**Challenges:**
- Log files can be gigabytes or terabytes in size
- Cannot open in standard text editors
- Need efficient search and filtering

**Techniques:**

**Command-Line Tools (Linux/Unix):**
```bash
# Search for specific pattern
grep "Failed password" /var/log/auth.log

# Count occurrences
grep -c "error" /var/log/syslog

# Search multiple files
grep -r "suspicious_pattern" /var/log/

# Extract specific time range (if ISO format)
awk '/2025-10-06T13:00/,/2025-10-06T14:00/' logfile.log

# Get unique source IPs from web log
awk '{print $1}' access.log | sort | uniq -c | sort -rn

# Filter and count by HTTP status code
awk '{print $9}' access.log | sort | uniq -c
```

**PowerShell (Windows):**
```powershell
# Search event logs
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4625} | 
    Where-Object {$_.TimeCreated -gt (Get-Date).AddDays(-1)}

# Count events by event ID
Get-WinEvent -LogName Security | Group-Object Id -NoElement | Sort-Object Count -Descending

# Export filtered results
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4624,4625} | 
    Select-Object TimeCreated, Id, Message | 
    Export-Csv C:\analysis\login_events.csv
```

**Streaming/Chunking:**
- Process logs in chunks rather than loading entire file
- Use tools designed for big data (Hadoop, Spark)
- Stream data through analysis pipelines

**Indexing:**
- Use log management platforms that index on ingestion
- Dramatically speeds up searching
- Enables complex queries across large datasets

---

## Module 4: Analyzing Specific Log Types

### 4.1 Firewall Log Analysis

**Key Information in Firewall Logs:**
- Connection attempts (allowed and blocked)
- Source and destination (IP, port, protocol)
- Direction (inbound/outbound)
- Firewall rule that processed the traffic
- Bytes transferred
- Session duration

**Analysis Focus Areas:**

**1. Unauthorized Access Attempts:**
- Look for blocked connections to sensitive ports (22, 23, 3389, 445)
- Identify port scanning patterns (sequential port attempts from same IP)
- Check for connections from unexpected geographic locations

**2. Policy Violations:**
- Outbound connections to blacklisted IPs
- Use of prohibited protocols
- Data transfers during maintenance windows

**3. Potential Data Exfiltration:**
- Large outbound transfers to external IPs
- Connections to file-sharing sites or cloud storage
- Unusual protocols for data transfer (DNS tunneling)

**Example Analysis Query:**
```
# Find all blocked inbound SSH attempts
source_ip != internal_network AND dest_port=22 AND action=deny

# Identify top blocked source IPs
action=deny | stats count by source_ip | sort -count
```

**Connection to Main Topic:** Firewall logs provide network perimeter security evidence. They show what traffic was allowed or denied, helping forensic investigators understand attack vectors and establish whether network-based controls were functioning properly during an incident.

### 4.2 Web Server Log Analysis

**Apache/Nginx Access Log Fields:**
- Client IP address
- Request timestamp
- HTTP method (GET, POST, PUT, DELETE)
- Requested URL and query parameters
- HTTP status code
- Bytes sent
- Referer header
- User-Agent string

**Common Attack Indicators:**

**SQL Injection:**
```bash
# Search for SQL injection patterns
grep -iE "(union.*select|concat.*0x|benchmark\()" access.log
grep -E "'.*OR.*'1'='1" access.log
```

**Cross-Site Scripting (XSS):**
```bash
# Look for script injection attempts
grep -iE "(<script|javascript:|onerror=|onload=)" access.log
```

**Directory Traversal:**
```bash
# Search for path traversal attempts
grep -E "\.\./|\.\.%2F|\.\.%5C" access.log
```

**Brute Force on Web Forms:**
```bash
# Count POST requests to login page by IP
grep "POST /login" access.log | awk '{print $1}' | sort | uniq -c | sort -rn
```

**Analysis Example:**

Investigating a web compromise:
```bash
# Extract all requests from suspicious IP
grep "203.0.113.45" access.log > suspicious_ip_activity.log

# Identify which URLs were accessed
awk '{print $7}' suspicious_ip_activity.log | sort | uniq

# Check for successful logins (status 200 on login page)
grep "POST /login.php\" 200" access.log

# Find all file uploads
grep "POST /upload" access.log

# Correlate with error log for application errors
grep "203.0.113.45" error.log
```

**Connection to Main Topic:** Web servers are common attack targets. Their logs provide crucial evidence of web-based attacks, user activity, and application-level security incidents. They're essential for investigating website defacements, data breaches via web applications, and DDoS attacks.

### 4.3 Windows Event Log Analysis

**Critical Event IDs for Forensics:**

**Account Logon Events (Domain):**
- 4768: Kerberos TGT requested
- 4769: Kerberos service ticket requested
- 4771: Kerberos pre-authentication failed

**Logon Events (Local):**
- 4624: Successful logon
  - Type 2: Interactive (console)
  - Type 3: Network (file share access)
  - Type 7: Unlock
  - Type 10: Remote Desktop
- 4625: Failed logon attempt
- 4634/4647: Logoff
- 4648: Explicit credential logon (RunAs)

**Account Management:**
- 4720: User account created
- 4722: User account enabled
- 4724: Password reset attempted
- 4728: User added to global group
- 4732: User added to local group
- 4756: User added to universal group

**Process Tracking (requires audit policy):**
- 4688: Process created
- 4689: Process exited

**Object Access:**
- 4663: Object accessed (file, folder)
- 4656: Handle to object requested
- 5140: Network share accessed

**System Events:**
- 7045: Service installed
- 7040: Service start type changed
- 1102: Audit log cleared (suspicious!)
- 4719: System audit policy changed

**Analysis Example Using PowerShell:**

```powershell
# Find all failed logon attempts in last 24 hours
Get-WinEvent -FilterHashtable @{
    LogName='Security'
    ID=4625
    StartTime=(Get-Date).AddDays(-1)
} | Select-Object TimeCreated, 
    @{Name='Username';Expression={$_.Properties[5].Value}},
    @{Name='SourceIP';Expression={$_.Properties[19].Value}} |
    Group-Object Username | 
    Sort-Object Count -Descending

# Find successful logons after failed attempts (potential compromise)
$FailedLogons = Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4625}
$SuccessLogons = Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4624}

# Find account creations
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4720} |
    Select-Object TimeCreated, 
    @{Name='NewAccount';Expression={$_.Properties[0].Value}},
    @{Name='CreatedBy';Expression={$_.Properties[4].Value}}

# Detect potential lateral movement (network logons)
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4624} |
    Where-Object {$_.Properties[8].Value -eq 3} |  # Logon Type 3
    Select-Object TimeCreated,
    @{Name='Account';Expression={$_.Properties[5].Value}},
    @{Name='SourceIP';Expression={$_.Properties[18].Value}}
```

**Red Flags in Windows Event Logs:**
- Event ID 1102 (audit log cleared) - possible cover-up attempt
- Multiple 4625 (failed logons) followed by 4624 (success) - brute force success
- 4720 (account creation) + 4732 (added to Administrators) in quick succession
- 4688 (process creation) showing suspicious tools (psexec, mimikatz, netcat)
- 4648 (explicit credentials) with service accounts - potential credential theft
- Logons (4624) at unusual times (3 AM) or from unusual locations

**Connection to Main Topic:** Windows Event Logs are the primary source of forensic evidence on Windows systems. They provide detailed audit trails of user activities, system changes, and security events. Proper analysis can reveal the full scope of a compromise, from initial access to data exfiltration.

### 4.4 Linux/Unix Log Analysis

**Key Log Files:**

**/var/log/auth.log (Debian) or /var/log/secure (RHEL):**
- Authentication attempts
- sudo usage
- SSH connections
- User account changes

**/var/log/syslog or /var/log/messages:**
- General system messages
- Service starts/stops
- Kernel messages
- Hardware events

**Analysis Techniques:**

**SSH Brute Force Detection:**
```bash
# Count failed SSH attempts by IP
grep "Failed password" /var/log/auth.log | awk '{print $(NF-3)}' | sort | uniq -c | sort -rn

# Find successful SSH logins
grep "Accepted password" /var/log/auth.log

# Identify successful logins after failures (possible compromise)
grep -E "(Failed|Accepted) password" /var/log/auth.log | grep -B 5 "Accepted"
```

**Sudo Usage Analysis:**
```bash
# List all sudo commands executed
grep "sudo:" /var/log/auth.log | grep COMMAND

# Find privilege escalation attempts
grep "sudo:" /var/log/auth.log | grep -E "(root|NOPASSWD)"

# Identify users who successfully used sudo
grep "sudo:" /var/log/auth.log | awk '/COMMAND/ {print $6}' | sort | uniq
```

**User Account Changes:**
```bash
# Find new user additions
grep "useradd" /var/log/auth.log

# Find user deletions
grep "userdel" /var/log/auth.log

# Find password changes
grep "passwd" /var/log/auth.log
```

**System Events:**
```bash
# System reboots
grep "reboot" /var/log/syslog

# Service stops/starts
grep -E "start|stop|restart" /var/log/syslog

# Kernel messages (hardware issues, security events)
grep "kernel:" /var/log/syslog
```

**Systemd Journal Analysis:**
```bash
# View all logs since last boot
journalctl -b

# View logs for specific service
journalctl -u ssh.service

# View logs for specific time range
journalctl --since "2025-10-06 00:00:00" --until "2025-10-06 23:59:59"

# View logs with priority ERROR or higher
journalctl -p err

# Follow logs in real-time
journalctl -f

# Export to file
journalctl --since "2025-10-01" > /mnt/evidence/journal_october.log
```

**Connection to Main Topic:** Linux systems are prevalent in server environments and often targeted by attackers. Understanding Linux log analysis is essential for investigating web server compromises, database breaches, and infrastructure attacks. The decentralized nature of Linux logging requires familiarity with multiple log files and formats.

### 4.5 Database Log Analysis

**MySQL/MariaDB:**

**General Query Log (if enabled):**
- All queries executed
- Connection/disconnection events
- User who executed queries

**Slow Query Log:**
- Queries exceeding time threshold
- Potential DoS or inefficient queries

**Binary Log:**
- All database changes (INSERT, UPDATE, DELETE)
- Used for replication and point-in-time recovery
- Critical for forensics showing data modifications

**Analysis Example:**
```bash
# Extract all queries from specific user
grep "user=bob" mysql-general.log

# Find queries accessing sensitive tables
grep -i "SELECT.*FROM.*customers" mysql-general.log

# Identify DROP or DELETE operations
grep -iE "(DROP|DELETE)" mysql-general.log

# Extract queries within time window
awk '/2025-10-06 13:00/,/2025-10-06 14:00/' mysql-general.log
```

**Microsoft SQL Server:**

**Error Log:**
- Server start/stop
- Configuration changes
- Authentication failures
- Backup/restore operations

**SQL Server Audit (if configured):**
- Customizable auditing of database events
- SELECT, INSERT, UPDATE, DELETE tracking
- Schema changes
- Permission modifications

**Analysis Using T-SQL:**
```sql
-- Query audit logs
SELECT event_time, action_id, succeeded, server_principal_name, 
       statement, database_name
FROM sys.fn_get_audit_file('C:\SQLAudit\*.sqlaudit', DEFAULT, DEFAULT)
WHERE event_time BETWEEN '2025-10-06 13:00:00' AND '2025-10-06 14:00:00'
ORDER BY event_time;

-- Find failed login attempts
SELECT event_time, server_principal_name, client_ip
FROM sys.fn_get_audit_file('C:\SQLAudit\*.sqlaudit', DEFAULT, DEFAULT)
WHERE action_id = 'LGIF'  -- Login Failed
ORDER BY event_time DESC;

-- Identify data modifications on sensitive tables
SELECT event_time, server_principal_name, statement
FROM sys.fn_get_audit_file('C:\SQLAudit\*.sqlaudit', DEFAULT, DEFAULT)
WHERE database_name = 'CustomerDB' 
  AND object_name = 'CreditCards'
  AND action_id IN ('SL', 'UP', 'DL', 'IN')  -- SELECT, UPDATE, DELETE, INSERT
ORDER BY event_time;
```

**PostgreSQL:**

**Log Configuration (postgresql.conf):**
- `log_statement = 'all'` logs all SQL statements
- `log_connections = on` logs connection attempts
- `log_disconnections = on` logs disconnections
- `log_duration = on` logs statement duration

**Analysis Example:**
```bash
# Find all queries from specific user
grep "user=contractor" postgresql-*.log

# Identify long-running queries
grep "duration:" postgresql-*.log | awk -F'duration: ' '{print $2}' | sort -rn

# Extract failed authentication attempts
grep "FATAL:  password authentication failed" postgresql-*.log

# Find data modification operations
grep -iE "(INSERT|UPDATE|DELETE) INTO" postgresql-*.log
```

**Connection to Main Topic:** Database logs are critical for investigations involving data breaches, unauthorized data access, or fraud. They provide evidence of who accessed what data, when, and what operations were performed. In many cases, database logs are the only record of data exfiltration or modification.

---

## Module 5: Log Analysis Tools and Platforms

### 5.1 Command-Line Tools

**grep (Global Regular Expression Print):**
- Search for patterns in text files
- Most fundamental log analysis tool
- Fast and efficient for large files

**Common Usage:**
```bash
# Basic search
grep "error" logfile.log

# Case-insensitive search
grep -i "error" logfile.log

# Count matches
grep -c "failed login" auth.log

# Show line numbers
grep -n "ERROR" application.log

# Extended regex (OR operations)
grep -E "error|warning|critical" logfile.log

# Recursive search in directory
grep -r "suspicious_pattern" /var/log/

# Invert match (show lines that DON'T match)
grep -v "INFO" logfile.log | grep -v "DEBUG"

# Context (show surrounding lines)
grep -A 5 -B 5 "ERROR" logfile.log  # 5 lines after and before
```

**awk (Pattern Scanning and Processing):**
- Extract specific fields from logs
- Perform calculations on log data
- Filter and transform log entries

**Common Usage:**
```bash
# Print specific fields (space-delimited)
awk '{print $1, $4}' access.log

# Filter by field value
awk '$9 == 404' access.log  # Show only 404 errors

# Sum bytes transferred
awk '{sum += $10} END {print sum}' access.log

# Count unique values
awk '{print $1}' access.log | sort | uniq -c

# Custom field separator
awk -F',' '{print $1, $3}' csvfile.log
```

**sed (Stream Editor):**
- Transform log data
- Extract portions of log lines
- Replace patterns

**cut (Extract Fields):**
```bash
# Extract specific columns (comma-delimited)
cut -d',' -f1,3,5 data.csv

# Extract characters
cut -c1-10 logfile.log  # First 10 characters
```

**sort and uniq:**
```bash
# Sort and remove duplicates
sort logfile.log | uniq

# Count occurrences
sort logfile.log | uniq -c | sort -rn

# Show only duplicates
sort logfile.log | uniq -d
```

**Connection to Main Topic:** Command-line tools are essential for quick log triage and analysis, especially when dealing with Linux systems or when GUI tools aren't available. Mastery of these tools enables rapid investigation and can be crucial during incident response when time is critical.

### 5.2 Log Management Platforms

#### ELK Stack (Elasticsearch, Logstash, Kibana)

**Components:**
- **Elasticsearch:** Search and analytics engine (storage and querying)
- **Logstash:** Log collection and processing pipeline (ingestion)
- **Kibana:** Visualization and analysis interface (presentation)

**Use Cases:**
- Centralized logging for multiple systems
- Real-time log analysis
- Custom dashboards and visualizations
- Complex queries across massive datasets

**Basic Query Examples (Kibana Query Language):**
```
# Find failed SSH logins
event.action: "ssh_login" AND event.outcome: "failure"

# Search for specific IP address
source.ip: "203.0.113.45"

# Time range and field filtering
@timestamp >= "2025-10-06T00:00:00" AND 
@timestamp <= "2025-10-06T23:59:59" AND 
log.level: "ERROR"

# Wildcard searches
user.name: admin*

# Boolean operators
(status: 404 OR status: 500) AND url: "/api/*"
```

**Advantages:**
- Open source and highly customizable
- Scalable to petabytes of data
- Rich visualization capabilities
- Active community and ecosystem

#### Splunk

**Features:**
- Commercial SIEM platform
- Powerful search language (SPL)
- Pre-built applications for common log sources
- Machine learning for anomaly detection
- Extensive correlation capabilities

**SPL (Search Processing Language) Examples:**
```
# Basic search
index=firewall sourcetype=cisco:asa action=blocked

# Statistical analysis
index=web_logs | stats count by status | sort -count

# Timeline creation
index=* | transaction user maxspan=1h | table _time, user, duration

# Detect failed login followed by success
index=auth_logs (action=failure OR action=success) 
| transaction user maxspan=5m 
| search action=failure action=success

# Correlation across sources
index=firewall OR index=ids 
| stats values(action) as actions by src_ip 
| where mvcount(actions) > 1
```

**Advantages:**
- Comprehensive out-of-the-box functionality
- Excellent for correlation and analytics
- Strong enterprise support
- Extensive app marketplace

**Disadvantages:**
- Expensive licensing model
- Can be resource-intensive

#### Graylog

**Features:**
- Open source log management
- Built on Elasticsearch and MongoDB
- Web-based interface
- Real-time alerting
- Stream processing

**Advantages:**
- Free community edition
- Easier setup than ELK Stack
- Built-in alerting
- Good performance

### 5.3 Specialized Forensic Tools

#### Log2Timeline/Plaso

**Purpose:** Automated timeline creation from multiple log sources

**Features:**
- Supports 100+ log formats
- Creates super-timeline from all available logs
- Outputs to various formats (CSV, SQLite, Elasticsearch)

**Usage:**
```bash
# Create timeline from disk image
log2timeline.py timeline.plaso /mnt/evidence/disk.dd

# Process timeline to CSV
psort.py -o l2tcsv -w timeline.csv timeline.plaso

# Filter during processing
psort.py -o l2tcsv -w filtered.csv timeline.plaso "date > '2025-10-01'"
```

**Advantages:**
- Automates tedious timeline creation
- Handles many log formats automatically
- Integrates with other forensic tools

#### Timesketch

**Purpose:** Collaborative timeline analysis

**Features:**
- Web-based interface
- Import Plaso timelines
- Collaborative investigation
- Tagging and commenting
- Graph analysis

**Use Cases:**
- Team-based investigations
- Complex incident analysis requiring multiple analysts
- Long-term investigations requiring documentation

#### Chainsaw

**Purpose:** Rapid Windows Event Log analysis

**Features:**
- Fast searching of .evtx files
- Sigma rule detection
- Timeline generation
- Written in Rust (very fast)

**Usage:**
```bash
# Hunt for threats using Sigma rules
chainsaw hunt evtx_files/ -s sigma_rules/ --mapping mappings/

# Search for specific Event IDs
chainsaw search evtx_files/ -e 4624,4625,4688

# Generate timeline
chainsaw dump evtx_files/ --output timeline.csv
```

#### Eric Zimmerman Tools

**Key Tools for Log Analysis:**
- **EvtxECmd:** Parse Windows Event Logs
- **PECmd:** Parse Prefetch files (shows program execution)
- **LECmd:** Parse LNK files
- **JLECmd:** Parse Jump Lists

**Example:**
```bash
# Parse all EVTX files in directory
EvtxECmd.exe -d "C:\evidence\evtx\" --csv "C:\analysis\" --csvf results.csv
```

### 5.4 Scripting for Log Analysis

**Python Example - Parse Apache Logs:**
```python
import re
from collections import Counter
from datetime import datetime

# Apache log regex pattern
log_pattern = re.compile(
    r'(\S+) \S+ \S+ \[([\w:/]+\s[+\-]\d{4})\] "(\S+)\s?(\S+)?\s?(\S+)?" (\d{3}) (\S+)'
)

def parse_log(log_file):
    ips = []
    status_codes = []
    user_agents = []
    
    with open(log_file, 'r') as f:
        for line in f:
            match = log_pattern.match(line)
            if match:
                ip = match.group(1)
                status = match.group(6)
                
                ips.append(ip)
                status_codes.append(status)
    
    # Analysis
    print("Top 10 IP Addresses:")
    for ip, count in Counter(ips).most_common(10):
        print(f"  {ip}: {count} requests")
    
    print("\nHTTP Status Code Distribution:")
    for code, count in Counter(status_codes).most_common():
        print(f"  {code}: {count}")

# Usage
parse_log('/var/log/apache2/access.log')
```

**PowerShell Example - Analyze Failed Logons:**
```powershell
# Get failed logon events from last 24 hours
$FailedLogons = Get-WinEvent -FilterHashtable @{
    LogName='Security'
    ID=4625
    StartTime=(Get-Date).AddDays(-1)
}

# Parse and group by username
$Analysis = $FailedLogons | ForEach-Object {
    [PSCustomObject]@{
        Time = $_.TimeCreated
        Username = $_.Properties[5].Value
        SourceIP = $_.Properties[19].Value
        FailureReason = $_.Properties[8].Value
    }
}

# Show top failed accounts
$Analysis | Group-Object Username | 
    Select-Object Name, Count | 
    Sort-Object Count -Descending |
    Format-Table

# Show attempts by source IP
$Analysis | Group-Object SourceIP |
    Select-Object Name, Count |
    Sort-Object Count -Descending |
    Format-Table

# Export to CSV for further analysis
$Analysis | Export-Csv -Path "C:\analysis\failed_logons.csv" -NoTypeInformation
```

**Connection to Main Topic:** Scripting enables custom analysis tailored to specific investigation needs. While commercial tools are powerful, custom scripts can perform specialized analysis, process proprietary log formats, or automate repetitive tasks. Programming skills significantly enhance a forensic analyst's capabilities.

---

## Module 6: Advanced Topics and Best Practices

### 6.1 Log Integrity and Anti-Forensics

**Threats to Log Integrity:**

**Log Tampering:**
- Attackers modifying or deleting logs to cover tracks
- Event ID 1102 in Windows (audit log cleared) is a red flag
- Missing time periods in logs

**Detection Methods:**
1. **Hash Verification:** Compare current hash with baseline
2. **Timestamp Analysis:** Look for gaps or inconsistencies
3. **Cross-Reference:** Verify events across multiple systems
4. **Write-Once Storage:** Use WORM media or immutable storage

**Protective Measures:**
- Forward logs to centralized server in real-time
- Use TLS encryption for log transmission
- Implement log signing (digital signatures)
- Configure append-only log files
- Store logs on separate, secured systems

**Log Evasion Techniques:**
- Disabling logging services
- Flooding logs with noise to hide malicious activity
- Living-off-the-land binaries that generate minimal logs
- Exploiting log rotation to force deletion

**Connection to Main Topic:** Understanding anti-forensics techniques helps investigators recognize when logs have been tampered with and where to look for alternative evidence sources. Robust log protection is essential for maintaining forensic integrity.

### 6.2 Time Zone and Timestamp Challenges

**Common Issues:**
- Logs from different systems in different time zones
- Systems with incorrect time settings
- Daylight Saving Time transitions
- Timestamp format inconsistencies

**Best Practices:**

**Normalize to UTC:**
```python
from datetime import datetime, timezone
import pytz

# Convert local time to UTC
local_tz = pytz.timezone('America/Chicago')
local_time = datetime(2025, 10, 6, 13, 30, 0)
local_time = local_tz.localize(local_time)
utc_time = local_time.astimezone(pytz.UTC)

print(f"Local: {local_time}")
print(f"UTC: {utc_time}")
```

**Document Time Sources:**
- Note each system's configured time zone
- Verify NTP configuration
- Check system time against known good source
- Record any time discrepancies

**Handle Ambiguous Times:**
- Be cautious during DST transitions
- One hour can occur twice or be skipped
- Document assumptions made during analysis

**Connection to Main Topic:** Incorrect time correlation can lead to completely wrong conclusions about event sequences. Proper timestamp handling is fundamental to accurate forensic analysis.

### 6.3 Privacy and Legal Considerations

**Data Privacy Concerns:**
- Logs often contain PII (Personally Identifiable Information)
- GDPR, CCPA, and other regulations apply
- Need-to-know principle for access
- Data minimization in collection

**Legal Requirements:**
- Chain of custody documentation
- Non-repudiation (prove logs weren't altered)
- Compliance with discovery requests
- Data retention policies

**Best Practices:**
- Obtain proper authorization before analysis
- Document all actions taken
- Redact unnecessary PII in reports
- Secure storage of forensic evidence
- Follow organizational and legal procedures

### 6.4 Reporting and Documentation

**Effective Forensic Reports Should Include:**

1. **Executive Summary:** High-level findings for non-technical audience
2. **Scope:** What was examined, what wasn't
3. **Methodology:** Tools and techniques used
4. **Timeline:** Chronological sequence of events
5. **Findings:** Detailed evidence with citations to logs
6. **Indicators of Compromise (IoCs):** IPs, domains, file hashes, etc.
7. **Impact Assessment:** What was affected, data at risk
8. **Recommendations:** Security improvements, containment actions
9. **Appendices:** Raw data, detailed logs, tool outputs

**Documentation Best Practices:**
- Screenshot important findings
- Include log excerpts with timestamps
- Cite log file names and line numbers
- Maintain detailed notes during analysis
- Use consistent terminology
- Have report reviewed by peer

**Sample Finding Documentation:**
```
Finding: Unauthorized Access via Compromised Credentials

Evidence:
- Firewall Log (fw01_2025-10-06.log, lines 1523-1537): 
  Shows inbound connection from IP 203.0.113.45 to web server 10.0.0.5:443 
  at 2025-10-06 13:45:03 UTC
  
- Web Server Access Log (access.log, lines 45231-45242):
  Shows successful authentication by user 'contractor_bob' from 203.0.113.45
  at 2025-10-06 13:45:15 UTC
  
- Application Log (app.log, lines 8932-8945):
  Shows 'contractor_bob' accessed customer database table at 2025-10-06 13:46:22 UTC
  Query extracted 50,000 customer records including PII
  
- Database Log (mysql-general.log, lines 112453-112465):
  Shows SELECT query on customers table by user 'contractor_bob' 
  at 2025-10-06 13:46:22 UTC

Impact: Potential breach of 50,000 customer records containing names, addresses, 
and payment information.

Timeline: Initial access at 13:45:03, data exfiltration completed by 13:52:18
```

### 6.5 Continuous Improvement

**Building Log Analysis Skills:**
- Practice with publicly available datasets
- Participate in CTF competitions
- Build home lab for experimentation
- Read incident reports and case studies
- Stay current with new attack techniques

**Resources for Practice:**
- Security Onion (pre-configured SIEM lab)
- SANS Internet Storm Center logs
- Wireshark sample captures
- Public incident reports

**Connection to Main Topic:** Log analysis is both an art and a science. Effective analysts combine technical skills with critical thinking, pattern recognition, and attention to detail. Continuous practice and learning are essential for maintaining expertise in this rapidly evolving field.

---

## Module 7: Practical Case Studies

### Case Study 1: Web Server Compromise

**Scenario:** A company's e-commerce website was defaced. Investigate using web server logs to determine how the attacker gained access.

**Available Evidence:**
- Apache access logs (7 days)
- Apache error logs (7 days)
- Application logs
- Firewall logs

**Investigation Steps:**

1. **Identify Anomalies:**
```bash
# Look for SQL injection attempts
grep -iE "(union.*select|' or |concat\()" access.log

# Check for unusual POST requests
grep "POST" access.log | awk '{print $7}' | sort | uniq -c | sort -rn

# Find directory traversal attempts
grep -E "\.\./" access.log
```

2. **Timeline Creation:**
- Defacement noticed at 2025-10-06 15:30 UTC
- Work backward to find initial compromise
- Look for successful exploitation (HTTP 200 on suspicious requests)

3. **Findings:**
```
2025-10-06 14:23:15 - SQL injection attempt on /product.php?id=
2025-10-06 14:23:45 - HTTP 200 response (successful injection)
2025-10-06 14:24:12 - File upload to /uploads/ directory
2025-10-06 14:25:03 - Web shell access at /uploads/shell.php
2025-10-06 15:28:44 - Modification of index.html (defacement)
```

4. **Lessons:**
- Web application input validation failure
- Unrestricted file upload vulnerability
- Need for web application firewall

### Case Study 2: Insider Threat Data Exfiltration

**Scenario:** Confidential documents appeared on a competitor's website. Investigate potential insider threat.

**Available Evidence:**
- Windows Security Event Logs (domain controller and file server)
- Network firewall logs
- DLP logs
- File access audit logs

**Investigation Process:**

1. **Identify Accessed Files:**
```powershell
# Find access to confidential directory
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4663} |
    Where-Object {$_.Message -like "*Confidential_Projects*"} |
    Select-Object TimeCreated, 
        @{Name='User';Expression={$_.Properties[1].Value}},
        @{Name='Object';Expression={$_.Properties[6].Value}}
```

2. **Correlate with Network Activity:**
- Check firewall logs for large outbound transfers
- Review user's logon sessions
- Examine email logs for large attachments

3. **Timeline:**
```
2025-09-28 18:45 - Employee 'jsmith' logs in after hours
2025-09-28 18:47 - Access to \\fileserver\confidential\ProjectX\
2025-09-28 18:52 - Multiple file reads (4663 events)
2025-09-28 19:15 - Large HTTPS POST to external IP (firewall log)
2025-09-28 19:18 - DLP alert: sensitive data transmitted externally
```

4. **Conclusion:**
Employee accessed confidential files outside normal working hours and transmitted them externally via encrypted channel.

### Case Study 3: Ransomware Attack

**Scenario:** Multiple servers encrypted by ransomware. Determine initial infection vector and spread method.

**Investigation:**

1. **Identify Patient Zero:**
- Review antivirus logs for first detection
- Check email logs for phishing attempts
- Examine web proxy logs for malicious downloads

2. **Trace Lateral Movement:**
```powershell
# Find network logons (Type 3) across multiple systems
Get-WinEvent -FilterHashtable @{LogName='Security'; ID=4624} |
    Where-Object {$_.Properties[8].Value -eq 3} |
    Group-Object @{Expression={$_.Properties[5].Value}} |
    Where-Object {$_.Count -gt 10}
```

3. **Identify Encryption Activity:**
- Process creation events (4688) showing ransomware executable
- High volume of file modifications (4663)
- Service creation (7045) for persistence

4. **Timeline Reconstruction:**
```
2025-10-05 09:15 - User opens malicious email attachment
2025-10-05 09:16 - Malware executes, beacon to C2 server
2025-10-05 09:30 - Credential harvesting (mimikatz execution)
2025-10-05 10:00 - Lateral movement to file servers (SMB connections)
2025-10-05 10:15 - Ransomware payload deployment across network
2025-10-05 10:20 - Mass file encryption begins
```

**Connection to Main Topic:** Case studies demonstrate how theoretical knowledge applies to real-world investigations. They show the importance of correlating multiple log sources and thinking like both an attacker and a defender.

---

## Module 8: Reference Documentation

### 8.1 Official Documentation and Standards

**Log Formats and Standards:**
- RFC 5424 - The Syslog Protocol
  https://tools.ietf.org/html/rfc5424
- RFC 3164 - The BSD syslog Protocol
  https://tools.ietf.org/html/rfc3164
- Common Event Format (CEF) Guide
  https://www.microfocus.com/documentation/arcsight/

**Windows Event Logging:**
- Windows Security Audit Events
  https://docs.microsoft.com/en-us/windows/security/threat-protection/auditing/
- Ultimate Windows Security Event Log Encyclopedia
  https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/
- Windows Event Log Reference
  https://docs.microsoft.com/en-us/windows/win32/wes/windows-event-log

**Linux/Unix Logging:**
- rsyslog Documentation
  https://www.rsyslog.com/doc/
- systemd Journal Documentation
  https://www.freedesktop.org/software/systemd/man/systemd-journald.service.html
- Linux Audit Framework (auditd)
  https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/security_guide/chap-system_auditing

**Network Device Logging:**
- Cisco Syslog Configuration Guide
  https://www.cisco.com/c/en/us/td/docs/switches/lan/catalyst9300/software/release/16-12/configuration_guide/sys_mgmt/b_1612_sys_mgmt_9300_cg/configuring_system_message_logging.html
- Palo Alto Networks Logging and Monitoring
  https://docs.paloaltonetworks.com/pan-os/10-2/pan-os-admin/monitoring

### 8.2 Forensic Resources

**NIST Computer Forensics Guidelines:**
- NIST SP 800-86: Guide to Integrating Forensic Techniques into Incident Response
  https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-86.pdf
- NIST SP 800-92: Guide to Computer Security Log Management
  https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-92.pdf

**SANS Reading Room:**
- Log Management and Analysis Papers
  https://www.sans.org/white-papers/
- DFIR (Digital Forensics and Incident Response) Papers
  https://www.sans.org/blog/

**Books:**
- "The Practice of Network Security Monitoring" by Richard Bejtlich
- "Applied Network Security Monitoring" by Chris Sanders and Jason Smith
- "Windows Forensic Analysis" by Harlan Carvey
- "File System Forensic Analysis" by Brian Carrier
- "Practical Forensic Imaging" by Bruce Nikkel

### 8.3 Tools and Software

**Open Source Tools:**
- ELK Stack: https://www.elastic.co/elastic-stack
- Graylog: https://www.graylog.org/
- Plaso/Log2Timeline: https://github.com/log2timeline/plaso
- Timesketch: https://timesketch.org/
- Chainsaw: https://github.com/WithSecureLabs/chainsaw
- Eric Zimmerman Tools: https://ericzimmerman.github.io/

**Commercial Tools:**
- Splunk: https://www.splunk.com/
- LogRhythm: https://logrhythm.com/
- QRadar: https://www.ibm.com/qradar
- ArcSight: https://www.microfocus.com/en-us/cyberres/secops/arcsight-esm

**Utilities:**
- grep, awk, sed (command-line text processing)
- jq (JSON processor): https://stedolan.github.io/jq/
- Wireshark (network protocol analyzer): https://www.wireshark.org/
- NetworkMiner (network forensics): https://www.netresec.com/?page=NetworkMiner

### 8.4 Training and Certification Resources

**Professional Certifications:**
- GIAC Certified Forensic Analyst (GCFA)
  https://www.giac.org/certifications/certified-forensic-analyst-gcfa/
- GIAC Certified Incident Handler (GCIH)
  https://www.giac.org/certifications/certified-incident-handler-gcih/
- Certified Computer Examiner (CCE)
  https://www.isfce.com/cce.htm
- EnCase Certified Examiner (EnCE)
  https://www.opentext.com/products/encase-forensic

**Online Training:**
- SANS Institute Courses (FOR500, FOR508, SEC504)
  https://www.sans.org/cyber-security-courses/
- Cybrary - Free Security Training
  https://www.cybrary.it/
- Pluralsight - Log Analysis Courses
  https://www.pluralsight.com/
- Udemy - Digital Forensics Courses
  https://www.udemy.com/

**Free Practice Resources:**
- DFIR Madness Challenges
  https://dfirmadness.com/
- CyberDefenders - Blue Team Labs
  https://cyberdefenders.org/
- Boss of the SOC (BOTS) Dataset from Splunk
  https://www.splunk.com/en_us/blog/security/boss-of-the-soc-scoring-server-questions-and-answers-and-dataset-open-sourced-and-ready-for-download.html

### 8.5 Community and Forums

**Online Communities:**
- r/netsec, r/computerforensics, r/cybersecurity (Reddit)
- SANS Internet Storm Center Forums
  https://isc.sans.edu/forums/
- Digital Forensics Discord Communities
- Volatility Plugin Contest Community
  https://volatility-labs.blogspot.com/

**Blogs and News:**
- Krebs on Security: https://krebsonsecurity.com/
- SANS Reading Room: https://www.sans.org/white-papers/
- DFIR Review: https://dfir.training/
- 13Cubed (YouTube Channel): https://www.youtube.com/c/13Cubed

### 8.6 Vendor-Specific Documentation

**Web Servers:**
- Apache HTTP Server Logs: https://httpd.apache.org/docs/current/logs.html
- Nginx Logging: https://nginx.org/en/docs/http/ngx_http_log_module.html
- IIS Logging: https://docs.microsoft.com/en-us/iis/manage/provisioning-and-managing-iis/configure-logging-in-iis

**Databases:**
- MySQL Logging: https://dev.mysql.com/doc/refman/8.0/en/server-logs.html
- PostgreSQL Logging: https://www.postgresql.org/docs/current/runtime-config-logging.html
- SQL Server Audit: https://docs.microsoft.com/en-us/sql/relational-databases/security/auditing/sql-server-audit-database-engine

**Cloud Platforms:**
- AWS CloudTrail: https://docs.aws.amazon.com/cloudtrail/
- Azure Monitor Logs: https://docs.microsoft.com/en-us/azure/azure-monitor/logs/
- Google Cloud Logging: https://cloud.google.com/logging/docs


---

### Additional Learning Paths

- **Network Forensics:** Deep dive into packet analysis and network traffic
- **Memory Forensics:** Analyzing RAM dumps for malware and artifacts
- **Mobile Forensics:** iOS and Android device investigation
- **Cloud Forensics:** AWS, Azure, and GCP log analysis
- **Malware Analysis:** Reverse engineering and behavioral analysis

